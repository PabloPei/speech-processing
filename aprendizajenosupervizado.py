# -*- coding: utf-8 -*-
"""Entrega_Peiretti_aprendizajenosupervizado (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mtbGwMk7BnHJpLQ4gjRvTVt-92b_Rnh2

---

---

<center> <h1>Procesamiento del Habla</h1> </center>
<center> <h1>Aprendizaje no Supervisado</h1> </center>

<center> <h2>Pablo Peiretti - 103592</h2> </center>


</div>

---

---
"""

#Modulo de importacion de librerias
import matplotlib.pyplot as plt
from matplotlib import patches
import numpy as np
import scipy
import numpy.fft as ft
from scipy import signal as sig
from scipy import io
import scipy.io.wavfile as wav
from math import *
from scipy import signal
import IPython.display as ipd
from IPython.core.display import Javascript
from scipy import stats as sps
from matplotlib import cm
from matplotlib import colors
import random
import math as math

"""---

#**1. Entrenamiento: implementación del algoritmo EM**

a) Inicializar elegiendo tres puntos para inicializar las medias de cada clase que sí se consideran etiquetados. Para las covarianzas elegir la covarianza interclase de los 9 puntos. Realizar un gráfico de los
9 puntos etiquetados y las medias y covarianzas iniciales.

b) Realizar las iteraciones EM, generando un gráfico por iteración donde se grafiquen cada punto con
el color obtenido por la “paleta RGB” de la responsabilidad (paso E), y las medias y covarianzasdibujadas mediante la elipse de nivel (paso M).

c) Realizar un gráfico alfinal de las iteraciones donde se vea la evolución del valor del loglikelihood en función del número de iteración.

## **Recopilación de datos**

- Se recopilan los datos a usados a lo largo del proyecto que pertenecen a los archivos "a.txt" , "b.txt" y "c.txt" \

- Se dividen en dos grupos, uno de entrenamiento que consta de 35 muestras por cada archivo y otro de prueba de 15 muestras por archivo

- Se muestran los datos divididos por fonema
"""

#defino la cantidad de datos y clases
cant_clases = 3
cant_data_ini = 3
cant_data_train_xarchivo = 35
cant_data_test = 15

#cargo los datos
data_x1 = np.loadtxt('./c1.txt')
data_x2 = np.loadtxt('./c2.txt')
data_x3 = np.loadtxt('./c3.txt')

"""
-----------------------------------------ini_conjuntos----------------------------------------------------------

Inicializa los conjuntos de entrenamiento y test (los recorta con Ntest/Ntrain datos y los mixea)

Args:
  data: Contiene los datos de los formantes. Array de [cantidad de datos filas X cantidad de formantes columnas]
  Ntrain: Cantidad de puntos de entrenamiento.  Integrer
  Ntest: Cantidad de puntos de test. Integrer

Return:
  conj_entrn: Corresponde al conjunto entrenamiento o train. Array de  [Ntrain filas X cantidad de formantes columnas]
  conj_prueb: Corresponde al conjunto test. Array de  [Ntest filas X cantidad de formantes columnas]

"""

def ini_conjuntos(data,Ntrain,Ntest):
  Ni = data.shape[0]
  inda = list(np.random.permutation(range(Ni)))
  conj_entrn = data[inda[0:Ntrain], 0:2]
  conj_prueb = data[inda[Ntrain:Ntrain+Ntest],0:2]
  return [conj_entrn, conj_prueb]

#Recopilo los conjuntos
[conjunto_entrenamiento_x1, conjunto_prueba_x1] = ini_conjuntos(data_x1,cant_data_train_xarchivo,cant_data_test)
[conjunto_entrenamiento_x2, conjunto_prueba_x2] = ini_conjuntos(data_x2,cant_data_train_xarchivo,cant_data_test)
[conjunto_entrenamiento_x3, conjunto_prueba_x3] = ini_conjuntos(data_x3,cant_data_train_xarchivo,cant_data_test)

#Grafico las muestras de entrenamiento
fig = plt.figure(figsize=(10,6))
fig.tight_layout()

plt.subplot(1,1,1)
plt.scatter(conjunto_entrenamiento_x1[:,0], conjunto_entrenamiento_x1[:,1], alpha = 0.5, color = 'red')
plt.scatter(conjunto_entrenamiento_x2[:,0], conjunto_entrenamiento_x2[:,1], alpha = 0.5, color = 'green')
plt.scatter(conjunto_entrenamiento_x3[:,0], conjunto_entrenamiento_x3[:,1], alpha = 0.5, color = 'blue')
plt.xlabel("Frecuencia 1 [Hz]")
plt.ylabel("Frecuencia 2 [Hz]")
plt.title("Muestras",fontsize=18)
plt.grid()
plt.legend(['/X1/', '/X2/', '/X3/'])

"""## **Algoritmo EM:**

- Se definen los pasos E y M del algoritmo EM

- Se define la función que calcula el log-likelihood
"""

"""
-----------------------------------------------paso_e--------------------------------------------

Realiza el paso E del algoritmo de EM ( Evalua p(Z|X, θold) )

Args:
  data: Contiene los datos de los formantes. Array de [cantidad de datos filas X cantidad de formantes columnas]
  mu: Contiene la informacion de la esperanza. Array de [cantidad de clases filas X cantidad de formantes columnas]
  clases: Cantidad de clases. Integrer
  pik: P(z = k). Array de [1 fila X cantidad de clases columnas]
  sigma: Matrices de covarianza. Array de [cantidad de clases X cantidad de formantes X cantidad de formantes]
  cantidad_tot: Cantidad total de datos de entrenamiento. Integrer

Return:
  gamma: Responsabilidades. Array de [cantidad de datos filas X cantidad de clases columnas]

"""

def paso_e(data,mu,pik,clases,sigma,cantidad_tot):
  gamma = np.zeros((cantidad_tot, clases))
  for n in range(cantidad_tot):
    for k in range(clases):
      gamma[n,k] = pik[k] * sps.multivariate_normal(mu[k],sigma[k]).pdf(data[n,:])

  gamma = gamma/np.sum(gamma,axis=1)[:, np.newaxis]

  return gamma

"""
-----------------------------------------------paso_m--------------------------------------------

Realiza el paso M del algoritmo de EM (Determina θnew)

Args:
  data: Contiene los datos de los formantes. Array de [cantidad de datos filas X cantidad de formantes columnas]
  mu: Contiene la informacion de la esperanza. Array de [cantidad de clases filas X cantidad de formantes columnas]
  clases: Cantidad de clases. Integrer
  sigma: Matrices de covarianza.  Array de [cantidad de clases X cantidad de formantes X cantidad de formantes]
  cantidad_tot: Cantidad total de datos de entrenamiento. Integrer

Return:
  Devuelve los valores actualizados de los arrays mu, sigma, pik

"""

def paso_m(data,mu,clases,sigma,cantidad_tot,gamma):
  nk = np.sum(gamma, axis = 0)
  pik = nk/cantidad_tot

  mu.fill(0)
  aux = np.zeros((2,2))
  sigma = [aux, aux, aux]

#Calculo Mu
  for m in range(clases):
    for l in range(cantidad_tot):
      mu[m] = mu[m] + (gamma[l,m])*data[l] #ojaldre con esto
    mu[m] = mu[m]/nk[m]

#Calculo Sigma
  for j in range(clases):
    for k in range(cantidad_tot):
      sigma[j] = sigma[j] + (gamma[k,j])*np.outer(data[k] - mu[j], data[k] - mu[j])
    sigma[j] = sigma[j]/nk[j]

  return mu, sigma, pik

"""
-----------------------------------------------likelihood--------------------------------------------

Calcula el log-likelihood para un conjunto de datos y parametros dados

Args:
  data: Contiene los datos de los formantes. Array de [cantidad de datos filas X cantidad de formantes columnas]
  mu: Contiene la informacion de la esperanza. Array de [cantidad de clases filas X cantidad de formantes columnas]
  clases: Cantidad de clases. Integrer
  pik: P(z = k). Array de [1 fila X cantidad de clases columnas]
  sigma: Matrices de covarianza. Array de [cantidad de clases X cantidad de formantes X cantidad de formantes]
  cantidad_tot: Cantidad total de datos de entrenamiento. Integrer

Return:
  like_hood: Log-likehood para el conjuto de datos y parametro dado en la entrada como data. Integrer

"""

def likelihood(data,mu,sigma,pik,clases,cantidad_tot):

  like_hood = 0

  for i in range(cantidad_tot):
    aux=0

    for j in range(clases):
      aux = aux + pik[j] * sps.multivariate_normal(mu[j],sigma[j]).pdf(data[i,:])

    like_hood = like_hood + np.log(aux)

  return like_hood

"""## **Inicialización de parametros**

- Se inicializa el conjunto de entrenamiento de 92 datos

- Se utilizan 9 puntos tomando como conocida la clase a la cual pertenecen (3 de cada archivo) para calcular un valor incial de mu y sigma


"""

#Inicializo el conjunto de entrenamiento
conjunto_entrenamiento = np.concatenate((conjunto_entrenamiento_x1[3:,:], conjunto_entrenamiento_x2[3:,:], conjunto_entrenamiento_x3[3:,:]), axis = 0 )
cant_data_train=len(conjunto_entrenamiento)

#Inicializo pi_k
pi_k = [1/cant_clases, 1/cant_clases, 1/cant_clases]

#Inicializo mu
mu = np.zeros((3,2))
mu[0]=np.mean(conjunto_entrenamiento_x1[0:cant_data_ini,:], axis=0)
mu[1]=np.mean(conjunto_entrenamiento_x2[0:cant_data_ini,:], axis=0)
mu[2]=np.mean(conjunto_entrenamiento_x3[0:cant_data_ini,:], axis=0)

#Inicializo gamma
gamma = np.zeros((cant_data_train,cant_clases))

#Inicializo sigma
sigma = np.zeros((cant_clases,2,2))

cov_x1 = np.cov(conjunto_entrenamiento_x1[0:cant_data_ini,:], rowvar=False)
cov_x2 = np.cov(conjunto_entrenamiento_x2[0:cant_data_ini,:], rowvar=False)
cov_x3 = np.cov(conjunto_entrenamiento_x3[0:cant_data_ini,:], rowvar=False)

covarianza = 1/cant_clases * (cov_x1 + cov_x2 + cov_x3)

sigma = [covarianza,covarianza,covarianza]

#inicializo el likelihood
likeli_hood=np.zeros(1)
#calculo el primer valor
likeli_hood[0]=likelihood(conjunto_entrenamiento, mu, sigma, pi_k, cant_clases, cant_data_train)

"""## **Gráficos iniciales:**

- Se realiza un gráfico de los 9 puntos etiquetados, las medias y covarianzas iniciales.
"""

u1 = np.cos(np.arange(0,2*pi,0.01))
u2 = np.sin(np.arange(0,2*pi,0.01))

A_menos1 = np.linalg.cholesky(sigma[0])
V = np.transpose(A_menos1.dot(np.matrix([u1,u2])))


fig = plt.figure(figsize=(10,6))
fig.tight_layout()

plt.subplot(1,1,1)

plt.plot(V[:,0]+mu[0,0],V[:,1]+mu[0,1],'r')
plt.plot(V[:,0]+mu[1,0],V[:,1]+mu[1,1],'g')
plt.plot(V[:,0]+mu[2,0],V[:,1]+mu[2,1],'b')

plt.plot(mu[0,0], mu[0,1] , 'rx' )
plt.plot(mu[1,0], mu[1,1], 'gx')
plt.plot(mu[2,0], mu[2,1], 'bx')


plt.scatter(conjunto_entrenamiento_x1[0:cant_data_ini,0], conjunto_entrenamiento_x1[0:cant_data_ini,1], alpha = 0.5, color = 'red')
plt.scatter(conjunto_entrenamiento_x2[0:cant_data_ini,0], conjunto_entrenamiento_x2[0:cant_data_ini,1], alpha = 0.5, color = 'green')
plt.scatter(conjunto_entrenamiento_x3[0:cant_data_ini,0], conjunto_entrenamiento_x3[0:cant_data_ini,1], alpha = 0.5, color = 'blue')
plt.xlabel("Frecuencia 1 [Hz]")
plt.ylabel("Frecuencia 2 [Hz]")
plt.title("Datos, covarianzas y medias de Inicio",fontsize=18)
plt.grid()
plt.legend(['/X1/', '/X2/', '/X3/'])

plt.show()

"""## **Aplicación del algoritmo Expectation–Maximization**"""

gamma

#Defino el error del log-likelihood
cota_error_likehood=0.01



fig = plt.figure(figsize=(20,40))

fig.subplots_adjust(hspace=0.3)
fig.subplots_adjust(wspace=0.3)

fig.tight_layout()
fig.suptitle("\n\n    Datos de entrenamiento aprendizaje no supervisado",fontsize=40,va='center')

#ciclo do while mientras el likelihood no converga al valor buscado (En python no existe el do while, voy a tener que usar un while true con break)

iter=0
while True:

  iter= iter + 1
  gamma = paso_e(conjunto_entrenamiento, mu, pi_k, cant_clases, sigma, cant_data_train)
  mu, sigma, pi_k = paso_m(conjunto_entrenamiento, mu,cant_clases, sigma, cant_data_train, gamma)
  likeli_hood = np.append(likeli_hood, likelihood(conjunto_entrenamiento, mu, sigma, pi_k, cant_clases, cant_data_train))



  #si son mas de 20 iteraciones deja de graficar y solo calcula (para que no se rompa, ni se haga lento)
  if iter <= 20:

    plt.subplot(6,3,iter)

    for x in range(3):
      u1 = (x+1)*np.cos(np.arange(0,2*pi,0.01))
      u2 = (x+1)*np.sin(np.arange(0,2*pi,0.01))

      AX1_menos1 = np.linalg.cholesky(sigma[0])
      V_x1 = np.transpose(AX1_menos1.dot(np.matrix([u1,u2])))

      AX2_menos1 = np.linalg.cholesky(sigma[1])
      V_x2 = np.transpose(AX2_menos1.dot(np.matrix([u1,u2])))

      AX3_menos1 = np.linalg.cholesky(sigma[2])
      V_x3 = np.transpose(AX3_menos1.dot(np.matrix([u1,u2])))


      plt.plot(V_x1[:,0]+mu[0,0],V_x1[:,1]+mu[0,1],'r')
      plt.plot(V_x2[:,0]+mu[1,0],V_x2[:,1]+mu[1,1],'g')
      plt.plot(V_x3[:,0]+mu[2,0],V_x3[:,1]+mu[2,1],'b')


    plt.plot(mu[0][0], mu[0][1] , 'rx' )
    plt.plot(mu[1][0], mu[1][1], 'gx')
    plt.plot(mu[2][0], mu[2][1], 'bx')

    plt.xlabel("Frecuencia 1 [Hz]")
    plt.ylabel("Frecuencia 2 [Hz]")

    plt.title("Iteración %i" %iter)
    plt.scatter(conjunto_entrenamiento[:,0],conjunto_entrenamiento[:,1],alpha = 0.6, color=[gamma[k,:] for k in range(len(gamma))])
    plt.legend(['X1', 'X2', 'X3'])
    plt.grid()



  #CONDICION DE CORTE
  if abs(likeli_hood[iter] - likeli_hood[iter-1]) < cota_error_likehood:
      break

"""## **Gráfico del log-likelihood**"""

fig = plt.figure(figsize=(10,6))
fig.tight_layout()
plt.subplot(1,1,1)


iteraciones=np.arange(1,len(likeli_hood),1) #arranco desde la uno para verlo mejor por la escala
plt.scatter(iteraciones,likeli_hood[1:])
plt.xlabel("Iteración")
plt.ylabel("Amplitud")
plt.grid()
plt.title("Evolución del Log-likelihood",fontsize=18)
plt.show()

"""## **Testeo**

- Realiza el recuento de errores de testeo sobre los 45 puntos del conjunto de test dibujando para cada punto la etiqueta real y la obtenida con dos marcadores superpuestos
"""

"""
-----------------------------------------------decidir_clase--------------------------------------------

Calcula la probabilidad de que un punto pertenezca a cada una de las clases y devuelve la mas probable

Args:
  dato: Contiene la informacion de los formantes de un punto dado. Array de [1 fila X cantidad de formantes columnas]
  mult_clasek: Contiene la informacion de la multinomial para cada clase. Array de [Cantidad de clases fila x 1 columna]

Return:
  Devuelve el indice que corresponde a la clase que es mas probable que perteneza. Integrer

"""
def decidir_clase(dato, mult_clasek):

  proba_clase=[0,0,0]

  proba_clase[0]=mult_clasek[0].pdf(dato)
  proba_clase[1]=mult_clasek[1].pdf(dato)
  proba_clase[2]=mult_clasek[2].pdf(dato)

  return(np.argmax(proba_clase, axis=0))

"""
-----------------------------------------------calcular_resultado--------------------------------------------

Calcula el resultado del test sobre a que clase pertenece cada dato y los verdaderos valores

Args:
  mult_clasek: Contiene la informacion de la multinomial para cada clase. Array de [Cantidad de clases fila x 1 columna]
  conjunto_test: Contiene la informacion del conjunto de datos de prueba. Array de [cantidad datos test filas X cantidad formantes columnas]

Return:
  resultado_test_estimado: Conjunto resultado del test. Array de [cantidad datos test filas X cantidad formantes columnas]
  resultado_test_real: Conjunto de valores verdaderos. Array de [cantidad datos test filas X cantidad formantes columnas]

"""

def calcular_resultados(conjunto_test,mult_clasek):
  #calculo la estimacion para cada caso
  cantidad_test=len(conjunto_test)

  resultado_test_estimado = np.zeros((cantidad_test,cant_clases))
  for i in range(cantidad_test):
    clase_decidida=decidir_clase(conjunto_test[i],mult_clasek)
    resultado_test_estimado[i,clase_decidida]=1

  #calculo el resultado real
  resultado_test_real = np.zeros((cantidad_test,cant_clases))
  for i in range(cantidad_test):
    clase_decidida=int(i/15)
    resultado_test_real[i,clase_decidida]=1

  return resultado_test_estimado,resultado_test_real

"""
-----------------------------------------------calcular_error--------------------------------------------

Calcula la cantidad de errores que hubo entre los resultados del test y los valores reales

Args:
  resultado_test_estimado: Conjunto resultado del test. Array de [cantidad datos test filas X cantidad formantes columnas]
  resultado_test_real: Conjunto de valores verdaderos. Array de [cantidad datos test filas X cantidad formantes columnas]

Return:
 error: Cantidad de errores entre el resultado real y el estimado. Integrer

"""

def calcular_error(resu_estimado,resu_real):

  cantidad_test=len(conjunto_test)
  error=0
  for i in range(cantidad_test):
    if( not (np.array_equal(resu_estimado[i], resu_real[i]))):
      error=error+1

  return error

conjunto_test = np.concatenate((conjunto_prueba_x1, conjunto_prueba_x2, conjunto_prueba_x3), axis = 0 )

#defino la multivariable para cada clase con los parametros obtenidos en el entrenamiento
mult_clasek = []
for i in range(cant_clases):
  mult_clasek.append(sps.multivariate_normal(mu[i],sigma[i]))

#Calculo el resultado estimado y el real
[resultado_estimado, resultado_real] = calcular_resultados(conjunto_test, mult_clasek)

#calculo los errores
cantidad_errores=calcular_error(resultado_estimado, resultado_real)

fig = plt.figure(figsize=(25,10))
fig.tight_layout()
plt.subplot(1,2,1)

plt.scatter(conjunto_test[:,0],conjunto_test[:,1],s = 150, alpha=0.5,color=[resultado_real[k,:]for k in range(len(resultado_real))])
plt.scatter(conjunto_test[:,0],conjunto_test[:,1],s = 150, marker = 'x', alpha=1,color=[resultado_estimado[k,:]for k in range(len(resultado_estimado))])

plt.xlabel("Frecuencia 1 [Hz]")
plt.ylabel("Frecuencia 2 [Hz]")

#para que no diga 1 errores jaja
if cantidad_errores == 1:
  plt.title("Resultado de la estimación:  %i Error" %cantidad_errores ,fontsize=18)
else:
  plt.title("Resultado de la estimación:  %i Errores" %cantidad_errores ,fontsize=18)

plt.legend(['Valores reales', 'Valores estimados'])
plt.grid()

plt.subplot(1,2,2)
valoresa = [len(conjunto_test),cantidad_errores]
label = ["Correctas","Incorrectas"]
plt.pie(valoresa, labels=label,autopct="%0.1f %%")
plt.title("Resultados Test", fontsize=18)

plt.show()

"""## Gráfico Curvas de Decisión

- Se grafican las curvas de decisión obtenidas de acuerdo a las medias y covarianzas finales de entrenamiento utilizadas en el testeo, y los puntos de test con sus etiquetas reales
"""

fig = plt.figure(figsize=(10,6))
fig.tight_layout()
plt.subplot(1,1,1)

x1, x2 = np.meshgrid(np.arange(0, 2000, 1),np.arange(0, 2000,1))
pos = np.empty(x1.shape + (2, ))
pos[:,:,0] = x1
pos[:,:,1] = x2

ym = np.empty(x1.shape + (3,))
ym[:,:,0] = mult_clasek[0].pdf(pos)
ym[:,:,1] = mult_clasek[1].pdf(pos)
ym[:,:,2] = mult_clasek[2].pdf(pos)

argmax = np.argmax(ym, axis = 2)


colores_areas = ['r','r','g','g','g','b','b','b']
plt.contour(x1,x2,argmax,colors='pink')
cs=plt.contourf(x1,x2,argmax,colors=colores_areas,alpha=0.30)
plt.scatter(conjunto_test[:,0],conjunto_test[:,1],s = 150, alpha=0.5,color=[resultado_real[k,:]for k in range(len(resultado_real))],label='Valores reales test')
plt.scatter(conjunto_test[:,0],conjunto_test[:,1],s = 150, marker = 'x', alpha=1,color=[resultado_estimado[k,:]for k in range(len(resultado_estimado))], label='Valores estimados test')
plt.legend(['Valores reales', 'Valores estimados'])

plt.legend()
plt.xlabel("Frecuencia 1")
plt.ylabel("Frecuencia 2")
plt.title("Curvas de Decisión Obtenidas",fontsize=18)

plt.grid()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content/drive/My Drive/Colab Notebooks
# !sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic
# !jupyter nbconvert --to pdf Entrega-Peiretti-aprendizajenosupervizado.ipynb

